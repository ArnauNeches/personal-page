<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>COMP3888</title>
</head>

<body>
    <header>
        <h1>Accessible Activity Data Platform</h1>
        <p> This project was the <a href="https://www.sydney.edu.au/units/COMP3888">Computer Science Capstone
                Project</a> at The University of Sydney which I did during my exchange year.
            It was supposed to be done in a 7 people group project but as it always happens with these projects, it was
            just 2 of us did the whole job (It was a bit stressful, however, I learnt a lot).
        </p>
    </header>
    <main>
        <section>
            
            <article>
                <h2>About the project: </h2>
                <p>
                    I designed and built a microservice-based web app that automates the discovery and management of
                    accessible physical activity data for a real client. I led the project end-to-end, acting as project
                    manager, main developer and customer liaison (I had to organize meetings every week).
                </p>
                <p>
                    This replaces an excel spreadsheet process used previously by the client with a scalable backend, a
                    clean web
                    interface, and an AI-powered scraping pipeline. This pipeline automatically finds relevant activity
                    providers online and turns unstructured web content into structured data using independent service
                    modules built with Python, Flask, and SQLAlchemy.
                </p>
                <p>
                    The system integrates PostgreSQL for persistent storage, Pandas for data exploration and cleaning,
                    and
                    multiple external APIs including Google Maps API, Google Geocoding, and Google Custom Search.
                    Dynamic
                    content scraping is handled with Playwright and BeautifulSoup, while Gemini API is used to convert
                    raw text into structured, schema-compliant candidate records.
                </p>
                <p>
                    A candidate workflow ensures data quality before new entries are added to the main
                    dataset. The frontend is built with HTML, Jinja templates, JavaScript, and TailwindCSS.
                </p>
                <p>
                    This project demonstrates my ability to lead technical projects, design production-ready systems,
                    and deliver AI-driven automation across the full stack.
                </p>
            </article>
        </section>
        <section>
            <h2>The project: </h2>
            <p> Unfortunately since this project was done for a University course and had real clients involved, it
                cannot
                be shared freely. However I can show some video demonstrations of how it worked:
            </p>
            <!-- To be added -->
        </section>
    </main>
        <footer>
        <p>Last updated: <time datetime="2025-12-27">27/12/2025</time></p>
        <p>Back to main page: <a href="index.html">Main page</a></p>
    </footer>

</body>

</html>